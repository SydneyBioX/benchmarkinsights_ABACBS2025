# Interpreting benchmarking results with BenchmarkInsights

## Overview

In this workshop, we will work with a subset of benchmarking results from the SpatialSimBench study (<https://doi.org/10.1186/s13059-025-03505-w>). This dataset contains method performance scores across multiple simulation models, metrics, and dataset sizes, allowing us to explore trends, comparisons, and evaluation patterns using a variety of visual and statistical techniques.

### Pre-requisites

It is expected that students will have:

-   Basic knowledge of R syntax,
-   Familiarity with SingleCellExperiment and/or SpatialExperiment objects, and
-   Familiarity with fundamental concepts in data visualisation (e.g., boxplots, scatter plots) and simple statistical modelling.

### Time outline

The expected timing of the workshop:

| Activity           | Time            |
|--------------------|-----------------|
| Introduction       | 1:35pm - 1:50pm |
| Exploring the data | 1:50pm - 2:00pm |
| Break              | 2:00pm - 2:10pm |
| Interpret the data | 2:10pm - 2:45pm |
| Future work        | 2:45pm - 2:55pm |

### Learning objectives

-   Understand how to explore and summarise benchmark results across methods, datasets, and metrics.
-   Interpret variability, correlation, and scalability patterns across different methods.
-   Understand the key analytical steps involved in spatial omics analysis, and perform these steps using R.
-   Apply visual and statistical tools (boxplots, correlation plots, forest plots, scalability plots) to extract insights from benchmarking data.
-   Build intuition for how evaluation metrics respond to method differences and how these patterns can inform method selection.
